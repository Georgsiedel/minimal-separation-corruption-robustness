{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport math\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom joblib import Parallel, delayed\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import resample\nfrom scipy.stats import norm\nfrom sklearn.model_selection import train_test_split\nimport multiprocessing\nimport matplotlib.pyplot as plt\nimport time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-16T15:07:11.380472Z","iopub.execute_input":"2022-06-16T15:07:11.380745Z","iopub.status.idle":"2022-06-16T15:07:11.385712Z","shell.execute_reply.started":"2022-06-16T15:07:11.380717Z","shell.execute_reply":"2022-06-16T15:07:11.385021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate r-separation distance of dataset\ndef get_nearest_oppo_dist(X, y, norm, n_jobs):\n    if len(X.shape) > 2:\n        X = X.reshape(len(X), -1)\n    p = norm\n\n    def helper(yi):\n        return NearestNeighbors(n_neighbors=1, \n                                metric='minkowski', p=p, n_jobs=-1).fit(X[y != yi])\n\n    nns = Parallel(n_jobs=n_jobs)(delayed(helper)(yi) for yi in np.unique(y))\n    ret = np.zeros(len(X))\n    for yi in np.unique(y):\n        dist, _ = nns[yi].kneighbors(X[y == yi], n_neighbors=1)\n        ret[np.where(y == yi)[0]] = dist[:, 0]\n\n    return nns, ret","metadata":{"execution":{"iopub.status.busy":"2022-06-16T15:07:11.43433Z","iopub.execute_input":"2022-06-16T15:07:11.434915Z","iopub.status.idle":"2022-06-16T15:07:11.44159Z","shell.execute_reply.started":"2022-06-16T15:07:11.43488Z","shell.execute_reply":"2022-06-16T15:07:11.440875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sampling(position, label, distance, k_samples, seed):\n    np.random.seed(seed)\n    x_low = position - distance\n    x_high = position + distance\n    x_samples = np.random.uniform(x_low, x_high, (k_samples,2))\n    sample_list = []\n    \n    for x_sample in x_samples:\n        sample_list.append([x_sample[0], x_sample[1], label])\n\n    return sample_list","metadata":{"execution":{"iopub.status.busy":"2022-06-16T15:07:11.486872Z","iopub.execute_input":"2022-06-16T15:07:11.487268Z","iopub.status.idle":"2022-06-16T15:07:11.491928Z","shell.execute_reply.started":"2022-06-16T15:07:11.48724Z","shell.execute_reply":"2022-06-16T15:07:11.491382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# small k sampling can go wrong when in some seed all samples are excluded. e.g. k=1 will go wrong\ndef sampling_round(position, label, distance, k_samples, seed):\n    #position = np.array([1, 2])\n    #distance = 0.1\n    np.random.seed(seed)\n    x_low = position - distance\n    x_high = position + distance\n    x_samples = np.random.uniform(x_low, x_high, [k_samples,2])\n    df_samples = pd.DataFrame(x_samples) \n    round_samples = []\n    for x1,x2 in zip(df_samples[0],df_samples[1]):\n        d = math.sqrt((x1-position[0])*(x1-position[0])+(x2-position[1])*(x2-position[1]))\n        if d < distance:\n            round_samples.append([x1,x2,label])\n        \n    return round_samples\n    \n#s = sampling_round(np.array([1, 1]), 0, 0.1, 1000)\n#sdf = pd.DataFrame(s)\n#print(sdf)\n#plt.scatter(sdf.iloc[:,0], sdf.iloc[:,1], color='black')","metadata":{"execution":{"iopub.status.busy":"2022-06-16T15:07:11.542635Z","iopub.execute_input":"2022-06-16T15:07:11.543234Z","iopub.status.idle":"2022-06-16T15:07:11.549228Z","shell.execute_reply.started":"2022-06-16T15:07:11.543202Z","shell.execute_reply":"2022-06-16T15:07:11.548635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load training data from file\n#dataset = '../input/siemens-aida/trainingdata_a.csv'\n#df = pd.read_csv(dataset, sep=';')\n#x = df[['x_i1','x_i2']].to_numpy()\n#y = df['l_i'].to_numpy()\n\nwith np.load('./data_2d/B_sep.npz') as dataset:\n    x = dataset['x']\n    y = dataset['y']\nprint(\"Number of Datapoints in the set:%f\" % len(x))\n\ntime0 = time.perf_counter()\ndist = np.inf #1, 2, np.inf\nnns, ret = get_nearest_oppo_dist(x, y, dist, -1)\nprint(\"2R-Separation Minimal: %f\" % ret.min())\nprint(\"2R-Separation Mean: %f\" % ret.mean())\nepsilon = ret.min()/2\nprint(\"Epsilon: %f\" % epsilon)\ntime1 = time.perf_counter()\nprint(f\"Separation Calculation took {time1 - time0:0.3f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2022-06-16T15:07:11.588095Z","iopub.execute_input":"2022-06-16T15:07:11.588477Z","iopub.status.idle":"2022-06-16T15:07:12.623486Z","shell.execute_reply.started":"2022-06-16T15:07:11.588449Z","shell.execute_reply":"2022-06-16T15:07:12.622602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"timestart = time.perf_counter()\n\nmodel_epsilon = epsilon\neval_epsilons = [epsilon\n                ]\nks = [1, 2, 3, 5, 8, 10, 15, 20, 30, 40, 50]\nks_str = ', '.join(map(str, ks))\neval_epsilons_str = ', '.join(map(str, eval_epsilons))\navg_test_acc = np.empty([len(eval_epsilons), len(ks)])\nstd_test_acc = np.empty([len(eval_epsilons), len(ks)])\n\n#msu = \"minimal separation unrobustness\" = increase in error rate when adding random noise to the test data \n#in a way that classes stay separated\n#avg_msu = np.empty([len(model_epsilons)])\n#std_msu = np.empty([len(model_epsilons)])\n\nfor idm, k in enumerate(ks):\n    print(\"k =\", k)\n    \n    runs = 300\n    clearresult = np.empty([runs])\n    epsilonresult = np.empty([runs])\n    results_acc = np.empty([len(eval_epsilons),runs])\n\n    for seed in range(runs):\n\n        with np.load('./data_2d/B_sep.npz') as dataset:\n            x = dataset['x']\n            y = dataset['y']\n        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=seed)\n\n        clf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=seed) \n        #clf_aug = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=seed) \n        #clf = KNeighborsClassifier(n_neighbors = 2)\n        #clf_aug = KNeighborsClassifier(n_neighbors = 2)\n\n        #time2 = time.perf_counter()\n        if model_epsilon == 0:\n            clf.fit(x_train, y_train)  \n        else:\n            k_samples_train = k\n            aug_list_train = []\n            for p,y in zip(x_train,y_train):   \n                aug_list_train.extend(sampling(p, y, model_epsilon, k_samples_train, seed))    \n            df_aug_train = pd.DataFrame(aug_list_train)\n            x_aug_train = df_aug_train.iloc[:,0:2]\n            y_aug_train = df_aug_train.iloc[:,2]\n            clf.fit(x_aug_train, y_aug_train)\n\n        for ide, eval_epsilon in enumerate(eval_epsilons):\n\n            if eval_epsilon == 0:\n                acc = clf.score(x_test, y_test)\n                results_acc[ide, seed] = acc\n            else:\n                aug_list_test = []\n                k_samples_test = k\n                for p,y in zip(x_test,y_test):   \n                    aug_list_test.extend(sampling(p, y, eval_epsilon, k_samples_test, seed))    \n                df_aug_test = pd.DataFrame(aug_list_test)\n                x_aug_test = df_aug_test.iloc[:,0:2]\n                y_aug_test = df_aug_test.iloc[:,2]\n                acc = clf.score(x_aug_test, y_aug_test)\n                results_acc[ide, seed] = acc\n                \n    #extra for-loop to get and write down the mean over all runs for every evaluation epsilon of one model_epsilon before training the next model_epsilon:\n    for ide2, eval_epsilon2 in enumerate(eval_epsilons):\n        avg_test_acc[ide2, idm] = results_acc[ide2,:].mean()*100    \n        std_test_acc[ide2, idm] = results_acc[ide2,:].std()*100 \n        if eval_epsilon2 == 0:\n            clearresult = results_acc[ide2,:]\n        if eval_epsilon2 == epsilon:\n            epsilonresult = results_acc[ide2,:]\n            \n    #msu = \"minimal separation unrobustness\" = increase in error rate when adding random noise to the test data \n    # in a way that classes stay separated\n    #avg_msu[idm] = (epsilonresult.mean() - clearresult.mean())/clearresult.mean()*100\n    #std_msu[idm] = np.subtract(epsilonresult, clearresult).std()/clearresult.mean()*100\n    \n          \nprint(avg_test_acc)\nprint(std_test_acc)\n#print(avg_msu)\n#print(std_msu)\nnp.savetxt(\n    './avg_testacc_over_ks.csv',\n    avg_test_acc, fmt='%1.4f', delimiter=';', header='Networks trained with'\n    ' different numbers of augmented points per data point (k = {}) along columns THEN evaluated on training set using (epsilon = {}) '\n    ' along rows'.format(ks_str, eval_epsilons_str))\n\nnp.savetxt(\n    './std_testacc_over_ks.csv',\n    std_test_acc, fmt='%1.4f', delimiter=';', header='Networks trained with'\n    ' different numbers of augmented points per data point (k = {}) along columns THEN evaluated on training set using (epsilon = {}) '\n    ' along rows'.format(ks_str, eval_epsilons_str))\n","metadata":{"execution":{"iopub.status.busy":"2022-06-16T15:07:12.625283Z","iopub.execute_input":"2022-06-16T15:07:12.62554Z","iopub.status.idle":"2022-06-16T15:18:09.737831Z","shell.execute_reply.started":"2022-06-16T15:07:12.625487Z","shell.execute_reply":"2022-06-16T15:18:09.737036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#this plots the accuracy and standard deviation over all ks\n\nx = ks\ny1 = std_test_acc\ny2 = avg_test_acc\n\nax1 = plt.subplot(211)\nplt.scatter(x, y2)\nplt.ylabel(\"Test accuracy [%]\", labelpad=None)\nplt.title(\"Effect of parameter k on test accuracy\", pad=10)\nplt.yticks(np.arange(99, 101, 1))\nplt.tick_params('x', labelbottom=False)\n\nax2 = plt.subplot(212, sharex=ax1)\nplt.scatter(x, y1)\nplt.xlabel(\"Number of augmented points k\", labelpad=6)\nplt.ylabel(\"Standard deviation [%]\", labelpad=8)\nplt.yticks(np.arange(0, 0.35, 0.1))\nplt.savefig(\"effect_k.svg\",bbox_inches='tight')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T15:18:09.738968Z","iopub.execute_input":"2022-06-16T15:18:09.739253Z","iopub.status.idle":"2022-06-16T15:18:10.143191Z","shell.execute_reply.started":"2022-06-16T15:18:09.739172Z","shell.execute_reply":"2022-06-16T15:18:10.142417Z"},"trusted":true},"execution_count":null,"outputs":[]}]}